# -*- coding: utf-8 -*-
"""Fake_News_Detection_complete.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1paeI0FTyuVZ58djCe0G4tM0GZQ3wSjG2
"""

#from google.colab import drive
#drive.mount('/content/drive')
import pandas as pd
import numpy as np
#drive.mount("/content/drive", force_remount=True)

true = pd.read_csv("True.csv")
true

true['lable'] = 1
true

Fake = pd.read_csv("Fake.csv")
Fake

Fake['lable'] = 0
Fake

Fake.isnull().sum()

true.isnull().sum()

true.info()

# Combining True and Fake datasets
Combined_News = pd.concat([Fake, true], axis=0)
Combined_News

Combined_News.rename(columns = {'lable':'Category'}, inplace = True)
Combined_News

from nltk.stem import WordNetLemmatizer
import re

lemma=WordNetLemmatizer()
lemma

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stops = set(stopwords.words('english'))

print("List of Stop Words using NLTK")
print(len(stops))
print(stops)

print()

import spacy
#loading the english language small model of spacy
en = spacy.load('en_core_web_sm')
stopwords = en.Defaults.stop_words

print("List of Stop Words using Spacy")
print(len(stopwords))
print(stopwords)

#combining the stopword list
combined_Stopwords=set((set(stops)|set(stopwords)))
print("Length of Stop words from both NLTK & Spacy")
print(len(combined_Stopwords))

import nltk
nltk.download('wordnet')
#text cleaning function
def clean_text(text):

    string = ""

    #lower casing
    text=text.lower()

    #simplifying text
    text=re.sub(r"i'm","i am",text)
    text=re.sub(r"he's","he is",text)
    text=re.sub(r"she's","she is",text)
    text=re.sub(r"that's","that is",text)
    text=re.sub(r"what's","what is",text)
    text=re.sub(r"where's","where is",text)
    text=re.sub(r"\'ll"," will",text)
    text=re.sub(r"\'ve"," have",text)
    text=re.sub(r"\'re"," are",text)
    text=re.sub(r"\'d"," would",text)
    text=re.sub(r"won't","will not",text)
    text=re.sub(r"can't","cannot",text)

    #removing any special character
    text=re.sub(r"[-()\"#!@$%^&*{}?.,:]"," ",text)
    text=re.sub(r"\s+"," ",text)
    text=re.sub('[^A-Za-z0-9]+',' ', text)

    return ' '.join([lemma.lemmatize(word) for word in text.split() if word.lower() not in combined_Stopwords])

#cleaning the whole data
Combined_News["text"]=Combined_News["text"].apply(clean_text)

#filtered Text after removing all the stopwords
Combined_News["text"]

# Your existing code for preprocessing and splitting the data
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
# Create a pipeline with Word2Vec for text representation and a RandomForest classifier
from gensim.models import Word2Vec
from sklearn.ensemble import RandomForestClassifier

X=Combined_News["text"] #feature
y=Combined_News["Category"] # traget

# Split data into train, validation, and test sets
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)

# Define a function to tokenize and preprocess the text
def tokenize_text(text):
    return text.split()  # Simple tokenizer, you can customize this further

# Train Word2Vec model on your combined dataset
word2vec_model = Word2Vec(sentences=Combined_News["text"].apply(tokenize_text), vector_size=100, window=5, min_count=1, workers=4)

# Create a TfidfVectorizer using Word2Vec embeddings
class Word2VecVectorizer:
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.vector_size = word2vec.vector_size

    def fit(self, X, y):
        return self

    def transform(self, X):
        return np.array([np.mean([self.word2vec.wv[word] for word in tokenize_text(text) if word in self.word2vec.wv] or [np.zeros(self.vector_size)], axis=0) for text in X])

# Create a pipeline with Word2VecVectorizer and RandomForestClassifier
text_clf = Pipeline([
    ("w2v", Word2VecVectorizer(word2vec_model)),
    ("clf", RandomForestClassifier())
])

# Fit the model on the combined training and validation data
text_clf.fit(X_train_val, y_train_val)

# Make predictions on the test set
predictions_test = text_clf.predict(X_test)

# Calculate accuracy on the test set
accuracy_test = accuracy_score(y_test, predictions_test)
report_test = classification_report(y_test, predictions_test)
print("Test Set:")
print(f'Accuracy: {accuracy_test}')
print(report_test)

"""##**True Prediction**"""

text_clf.predict(['Following weeks of protests against police brutality and racial injustice, Washington state lawmakers have passed a package of police reform bills aimed at addressing concerns raised by demonstrators. The bills include measures to ban chokeholds, limit the use of tear gas and military equipment by law enforcement, and establish a new independent office to investigate police misconduct. Governor Jay Inslee has expressed his support for the reforms and is expected to sign the bills into law. Advocates see this as a step towards addressing systemic issues within law enforcement and promoting accountability and transparency.'])

"""##**Fake Prediction**"""

text_clf.predict(['Washington Governor Jay Inslee issued a statewide stay-at-home order on Monday in an effort to combat the spread of the coronavirus. The order requires all residents to stay home except for essential activities such as buying groceries, seeking medical care, or going to work at essential businesses. Inslee emphasized the importance of social distancing and urged Washingtonians to take the order seriously to protect themselves and others from the virus. The order is set to remain in effect for at least two weeks, with the possibility of extension depending on the situation.'])

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predictions_test)

from flask import Flask, render_template, request
import os

# Define the path to your templates folder
template_folder_path = os.path.abspath("C:/Users/Admin/Desktop/New folder")

# Create the Flask app instance and specify the template folder path
app = Flask(__name__, template_folder=template_folder_path)

# Route to render the home page
@app.route("/")
def index():
    return render_template("index.html")

# Route to handle form submission and display result
@app.route("/predict", methods=["POST"])
def predict():
    if request.method == "POST":
        # Get the news text from the form
        news_text = request.form["news_text"]

        # Perform your prediction here based on the news text
        # For demonstration purposes, let's assume a random prediction
        #prediction = "Real" if len(news_text) % 2 == 0 else "Fake"
        prediction = text_clf.predict([news_text])[0]
        result = "Fake" if prediction == 0 else "Real"
        # Render the result template with the prediction
        return render_template("result.html", prediction=prediction)

if __name__ == "__main__":
    app.run(debug=True)

